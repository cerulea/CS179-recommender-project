{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 179 Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyGM as gm\n",
    "\n",
    "# names of 500 movies to be rated:\n",
    "with open('top-names.txt') as f: names = f.read().split('\\n')\n",
    "    \n",
    "# ratings = int(2000 x 500) ratings of 500 movies by 2000 people; -1 = not rated\n",
    "ratings = np.loadtxt('top-ratings-missing.txt')\n",
    "nUsers,nMovies = ratings.shape\n",
    "\n",
    "X = (ratings >= 7).astype(int);   # did each user like the movie?  (binary)\n",
    "# (use any threshold you like, but \"7+\" might be \"worth recommending\"?)\n",
    "\n",
    "# Let's split into training & test:\n",
    "np.random.seed(0)\n",
    "pi = np.random.permutation(nUsers)\n",
    "iTr,iTe = pi[:int(nUsers*.7)], pi[int(nUsers*.7):]\n",
    "Xtr,Xte = X[iTr,:],X[iTe,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a prediction task, censor some entries in X[iTe,:]:\n",
    "Xte_missing = np.copy(Xte)\n",
    "Xte_missing[ np.random.random_sample(Xte.shape)<=.3 ] = -1  # discard 30% of observations\n",
    "\n",
    "# Now, we can measure the likelihood of Xte,\n",
    "# or predict Xte given Xte_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare our model's accuracy/ improvement by comparing Xtr's and Xte's LL value when calculated independently vs an Ising model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Independently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Independent model Train LL:  -284.862769233602\n",
      "Independent model Test  LL:  -285.486137585076\n"
     ]
    }
   ],
   "source": [
    "pXi = np.mean(1.*Xtr,axis=0);  # empirical probability of liking each movie individually\n",
    "model0 = gm.GraphModel( [gm.Factor([gm.Var(i,2)],[1-pXi[i],pXi[i]]) for i in range(nMovies)])\n",
    "\n",
    "# How well does our model fit the data?\n",
    "print(\"Independent model Train LL: \",np.mean([model0.logValue(x) for x in Xtr]))\n",
    "print(\"Independent model Test  LL: \",np.mean([model0.logValue(x) for x in Xte]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating using Ising model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ising learning algorithm: Find adjacency graph using penalized logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# for each Xi, estimate the neighborhood of Xi using L1-reg logistic regression:\n",
    "C = 1./100  # inverse regularization penalty: smaller => sparser weights\n",
    "nbrs,th_ij,th_i = [None]*nMovies, [None]*nMovies, np.zeros((nMovies,))\n",
    "Xtmp = np.copy(Xtr)  # we'll be messing with the matrix\n",
    "for i in range(nMovies):  \n",
    "    Xtmp[:,i] = 0.      # remove ourselves\n",
    "    lr = LogisticRegression(penalty='l1',C=C,solver='liblinear').fit(Xtmp,Xtr[:,i])\n",
    "    nbrs[i] = np.where(np.abs(lr.coef_) > 1e-6)[1]\n",
    "    th_ij[i]= lr.coef_[0,nbrs[i]]/2.\n",
    "    th_i[i] = lr.intercept_/2.\n",
    "    Xtmp[:,i] = Xtr[:,i]; # & restore after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average connectivity at C= 0.01 : \n",
      "1.096  +/-  2.137939194645161\n"
     ]
    }
   ],
   "source": [
    "print(\"Average connectivity at C=\",C,\": \")\n",
    "print(np.mean([len(nn) for nn in nbrs]),\" +/- \",np.std([len(nn) for nn in nbrs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect single-variable factors\n",
    "factors1 = [gm.Factor(gm.Var(i,2),[-t,t]).exp() for i,t in enumerate(th_i)]\n",
    "\n",
    "# Collect non-zero pairwise factors\n",
    "for i in range(nMovies):\n",
    "    for j,n in enumerate(nbrs[i]):\n",
    "        scope = [gm.Var(i,2),gm.Var(int(n),2)]\n",
    "        t = th_ij[i][j]\n",
    "        factors1.append( gm.Factor(scope, [[t,-t],[-t,t]]).exp() )\n",
    "\n",
    "# Build a model from the factors\n",
    "model1 = gm.GraphModel(factors1)\n",
    "model1.makeMinimal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Induced width of order found:  16\n"
     ]
    }
   ],
   "source": [
    "# How well does our model fit the data?  Now we need to know the partition function!\n",
    "# Maybe we can do it using variable elimination?  Depends on the treewidth.\n",
    "order,val = gm.eliminationOrder(model1, 'minfill')\n",
    "# (actually \"val\" here is an estimate of the required memory, so that's even better, but...)\n",
    "\n",
    "pt = gm.PseudoTree(model1,order)                   # find the tree width:\n",
    "print(\"Induced width of order found: \",pt.width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log partition f'n:  396.02023094234\n"
     ]
    }
   ],
   "source": [
    "# 16 is not too bad (2^16 = 65k), so we can do it:\n",
    "import pyGM.wmb\n",
    "jt = gm.wmb.JTree(model1, elimOrder=order)\n",
    "lnZ = jt.msgForward()\n",
    "print(\"Log partition f'n: \",lnZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Log-likelihood is slightly better than the independent model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg Ising Train LL:  -281.4238762978957\n",
      "LogReg Ising Test  LL:  -282.77572242876647\n"
     ]
    }
   ],
   "source": [
    "# Now we can calculate the actual log-likelihood of the data:\n",
    "print(\"LogReg Ising Train LL: \",np.mean([model1.logValue(x) for x in Xtr]) - lnZ)\n",
    "print(\"LogReg Ising Test  LL: \",np.mean([model1.logValue(x) for x in Xte]) - lnZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ising learning algorithm: Maximum likelihood parameters given adjecency graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ising learning algorithm: Maximum pseudolikelihood params given adjacency graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ising learning algorithm: Find both together using multiplicative weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating model quality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional(factor,i,x):\n",
    "    return factor.t[tuple(x[v] if v!=i else slice(v.states) for v in factor.vars)]\n",
    "\n",
    "def pseudolikelihood(model,X):\n",
    "    LL = np.zeros( X.shape )\n",
    "    for i in range(X.shape[1]):  # for each variable:\n",
    "        flist = model.factorsWith(i, copy=False)\n",
    "        for j in range(X.shape[0]):\n",
    "            pXi = 1.\n",
    "            for f in flist: pXi *= conditional(f,i,X[j])\n",
    "            LL[j,i] = np.log( pXi[X[j,i]]/pXi.sum() ); \n",
    "    return LL.sum(1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate each model, including the ones we make ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-285.4861375850758"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseudolikelihood(model0, Xte).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-274.69567095774784"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pseudolikelihood(model1, Xte).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions given the Ising model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_missing(model, Xobs):\n",
    "    m,n = Xobs.shape\n",
    "    Xhat = np.copy(Xobs);\n",
    "    for j in range(m):\n",
    "        x_obs = {i:Xobs[j,i] for i in range(n) if Xobs[j,i] >= 0}\n",
    "        x_unobs = [i for i in range(n) if Xobs[j,i] < 0]\n",
    "        cond = gm.GraphModel([f.condition(x_obs) for f in model.factorsWithAny(x_unobs)])\n",
    "        for x in cond.X:\n",
    "            if x.states == 0: x.states = 1;  # fix a bug in GraphModel behavior for missing vars...\n",
    "        jt = gm.wmb.JTree(cond, weights=1e-6) # 0: for maximization\n",
    "        x_hat = jt.argmax();\n",
    "        for i in x_unobs: Xhat[j,i] = x_hat[i]\n",
    "    return Xhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yerli\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3334: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\yerli\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "# Slow!  (Constructing lots of conditional models...)\n",
    "Xte_hat = impute_missing(model1, Xte_missing)\n",
    "print('Error rate:', np.mean( (Xte_hat!=Xte_missing)[Xte_hat<0] ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
